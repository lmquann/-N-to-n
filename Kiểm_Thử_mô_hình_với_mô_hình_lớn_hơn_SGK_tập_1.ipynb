{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lmquann/-N-to-n/blob/main/Ki%E1%BB%83m_Th%E1%BB%AD_m%C3%B4_h%C3%ACnh_v%E1%BB%9Bi_m%C3%B4_h%C3%ACnh_l%E1%BB%9Bn_h%C6%A1n_SGK_t%E1%BA%ADp_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Kh·ªüi t·∫°o m√¥ h√¨nh v√† th∆∞ vi·ªán c·∫ßn thi·∫øt ƒë·ªÉ x·ª≠ l√Ω\n"
      ],
      "metadata": {
        "id": "uXVEsy3MXqmj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install google-generativeai sentence-transformers transformers torch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wOWQWb50YBu0",
        "outputId": "99c0d1a1-00f9-4b22-8f91-6a19bb90851a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: google-generativeai in /usr/local/lib/python3.12/dist-packages (0.8.5)\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.12/dist-packages (5.1.1)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.57.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n",
            "Requirement already satisfied: google-ai-generativelanguage==0.6.15 in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (0.6.15)\n",
            "Requirement already satisfied: google-api-core in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (2.25.2)\n",
            "Requirement already satisfied: google-api-python-client in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (2.184.0)\n",
            "Requirement already satisfied: google-auth>=2.15.0 in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (2.38.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (5.29.5)\n",
            "Requirement already satisfied: pydantic in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (2.11.10)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (4.15.0)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.12/dist-packages (from google-ai-generativelanguage==0.6.15->google-generativeai) (1.26.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (1.16.2)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (0.35.3)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (11.3.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.20.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.4)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.6.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.4.0)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /usr/local/lib/python3.12/dist-packages (from google-api-core->google-generativeai) (1.70.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from google-auth>=2.15.0->google-generativeai) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from google-auth>=2.15.0->google-generativeai) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.12/dist-packages (from google-auth>=2.15.0->google-generativeai) (4.9.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (1.1.10)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2025.10.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: httplib2<1.0.0,>=0.19.0 in /usr/local/lib/python3.12/dist-packages (from google-api-python-client->google-generativeai) (0.31.0)\n",
            "Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from google-api-python-client->google-generativeai) (0.2.0)\n",
            "Requirement already satisfied: uritemplate<5,>=3.0.1 in /usr/local/lib/python3.12/dist-packages (from google-api-python-client->google-generativeai) (4.2.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic->google-generativeai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic->google-generativeai) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic->google-generativeai) (0.4.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->sentence-transformers) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
            "Requirement already satisfied: grpcio<2.0.0,>=1.33.2 in /usr/local/lib/python3.12/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.75.1)\n",
            "Requirement already satisfied: grpcio-status<2.0.0,>=1.33.2 in /usr/local/lib/python3.12/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.71.2)\n",
            "Requirement already satisfied: pyparsing<4,>=3.0.4 in /usr/local/lib/python3.12/dist-packages (from httplib2<1.0.0,>=0.19.0->google-api-python-client->google-generativeai) (3.2.5)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.12/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=2.15.0->google-generativeai) (0.6.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install faiss-cpu"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_0IcUoU7Y73f",
        "outputId": "12e3acf1-3e88-4f75-8a7d-8f77175f7f46"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting faiss-cpu\n",
            "  Downloading faiss_cpu-1.12.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (5.1 kB)\n",
            "Requirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.12/dist-packages (from faiss-cpu) (2.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from faiss-cpu) (25.0)\n",
            "Downloading faiss_cpu-1.12.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (31.4 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m31.4/31.4 MB\u001b[0m \u001b[31m36.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: faiss-cpu\n",
            "Successfully installed faiss-cpu-1.12.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_bBo4RsKXko8"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import faiss # Import th∆∞ vi·ªán FAISS\n",
        "import google.generativeai as genai\n",
        "\n",
        "# --- C·∫•u h√¨nh API Gemini ---\n",
        "genai.configure(api_key='AIzaSyD0zlXkgaLTTUIOrueypy-wAoZHmbCqyEY')\n",
        "\n",
        "# Kh·ªüi t·∫°o m√¥ h√¨nh Gemini 2.5 flash\n",
        "model = genai.GenerativeModel('gemini-2.5-flash')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. T·∫£i v√† x·ª≠ l√Ω d·ªØ li·ªáu (Chunking)"
      ],
      "metadata": {
        "id": "xpAym11BXuHA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_and_chunk_text(file_path, chunk_size=300, overlap=50):\n",
        "    \"\"\"\n",
        "    T·∫£i n·ªôi dung t·ª´ file vƒÉn b·∫£n v√† chia th√†nh c√°c chunks.\n",
        "    M·ªói chunk c·ªë g·∫Øng gi·ªØ m·ªôt ng·ªØ c·∫£nh nh·∫•t ƒë·ªãnh.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        with open(file_path, 'r', encoding='utf-8') as f:\n",
        "            full_text = f.read()\n",
        "    except FileNotFoundError:\n",
        "        print(f\"L·ªói: Kh√¥ng t√¨m th·∫•y file t·∫°i ƒë∆∞·ªùng d·∫´n {file_path}\")\n",
        "        return []\n",
        "\n",
        "    chunks = []\n",
        "    start_index = 0\n",
        "    while start_index < len(full_text):\n",
        "        end_index = min(start_index + chunk_size, len(full_text))\n",
        "        chunk = full_text[start_index:end_index]\n",
        "        chunks.append(chunk)\n",
        "\n",
        "        if end_index == len(full_text):\n",
        "            break\n",
        "\n",
        "        # ƒê·ªÉ √Ω ƒë·∫øn overlap, l√πi l·∫°i ƒë·ªÉ chunk ti·∫øp theo c√≥ m·ªôt ph·∫ßn tr√πng l·∫∑p\n",
        "        start_index += chunk_size - overlap\n",
        "        # ƒê·∫£m b·∫£o start_index kh√¥ng b·ªã √¢m\n",
        "        if start_index < 0:\n",
        "            start_index = 0\n",
        "\n",
        "    print(f\"‚úÖ ƒê√£ t·∫£i v√† t·∫°o {len(chunks)} chunks t·ª´ file.\")\n",
        "    return chunks"
      ],
      "metadata": {
        "id": "AmW0_CDYXuXY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. T·∫°o Embeddings"
      ],
      "metadata": {
        "id": "MGylnsFdYkvF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_embeddings(chunks, model_name='sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2'):\n",
        "    \"\"\"\n",
        "    T·∫°o vector embeddings cho danh s√°ch c√°c chunks vƒÉn b·∫£n.\n",
        "    \"\"\"\n",
        "    print(f\"‚è≥ ƒêang t·∫£i m√¥ h√¨nh SentenceTransformer: {model_name}...\")\n",
        "    try:\n",
        "        model = SentenceTransformer(model_name)\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå L·ªói khi t·∫£i m√¥ h√¨nh SentenceTransformer: {e}. Vui l√≤ng ki·ªÉm tra k·∫øt n·ªëi internet ho·∫∑c t√™n m√¥ h√¨nh.\")\n",
        "        return None\n",
        "\n",
        "    print(f\"‚è≥ ƒêang t·∫°o embeddings cho {len(chunks)} chunks...\")\n",
        "    # T·∫Øt progress bar ƒë·ªÉ console s·∫°ch h∆°n trong qu√° tr√¨nh ch·∫°y ch√≠nh\n",
        "    embeddings = model.encode(chunks, show_progress_bar=False)\n",
        "    print(\"‚úÖ ƒê√£ t·∫°o embeddings xong.\")\n",
        "    return embeddings"
      ],
      "metadata": {
        "id": "knizmJM_Yjgv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. L·∫≠p ch·ªâ m·ª•c v·ªõi FAISS"
      ],
      "metadata": {
        "id": "r1PuXx6aYtZJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_faiss_index(embeddings):\n",
        "    \"\"\"\n",
        "    T·∫°o m·ªôt ch·ªâ m·ª•c FAISS t·ª´ c√°c embeddings.\n",
        "    FAISS y√™u c·∫ßu embeddings ph·∫£i c√≥ ki·ªÉu d·ªØ li·ªáu float32.\n",
        "    \"\"\"\n",
        "    if embeddings is None or len(embeddings) == 0:\n",
        "        print(\"Kh√¥ng c√≥ embeddings ƒë·ªÉ t·∫°o ch·ªâ m·ª•c FAISS.\")\n",
        "        return None\n",
        "\n",
        "    dimension = embeddings.shape[1]\n",
        "    # IndexFlatL2 l√† ch·ªâ m·ª•c ƒë∆°n gi·∫£n, t√¨m ki·∫øm kho·∫£ng c√°ch Euclidean ch√≠nh x√°c.\n",
        "    # Ph√π h·ª£p cho 1060 chunks.\n",
        "    index = faiss.IndexFlatL2(dimension)\n",
        "    index.add(embeddings.astype('float32')) # ƒê·∫£m b·∫£o ki·ªÉu d·ªØ li·ªáu l√† float32\n",
        "    print(f\"‚úÖ ƒê√£ t·∫°o ch·ªâ m·ª•c FAISS v·ªõi {index.ntotal} vector.\")\n",
        "    return index"
      ],
      "metadata": {
        "id": "AGxUFVxhYr-a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Truy xu·∫•t v·ªõi FAISS"
      ],
      "metadata": {
        "id": "Ap_7pk_GY3gS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def retrieve_top_k_chunks_faiss(query_embedding, faiss_index, chunks, k=5):\n",
        "    \"\"\"\n",
        "    T√¨m ki·∫øm c√°c chunk g·∫ßn nh·∫•t b·∫±ng ch·ªâ m·ª•c FAISS.\n",
        "    \"\"\"\n",
        "    if faiss_index is None:\n",
        "        return []\n",
        "\n",
        "    # --- KH·∫ÆC PH·ª§C L·ªñI K QU√Å L·ªöN: ƒê·∫£m b·∫£o k kh√¥ng v∆∞·ª£t qu√° t·ªïng s·ªë vector ---\n",
        "    k = min(k, faiss_index.ntotal)\n",
        "    if k == 0: # Tr∆∞·ªùng h·ª£p kh√¥ng c√≥ vector n√†o trong index\n",
        "        return []\n",
        "    # -----------------------------------------------------------------------\n",
        "\n",
        "    # FAISS y√™u c·∫ßu query_embedding l√† m·ªôt m·∫£ng 2D (1, dimension) v√† ki·ªÉu float32\n",
        "    query_embedding_reshaped = query_embedding.reshape(1, -1).astype('float32')\n",
        "\n",
        "    # D: distances (kho·∫£ng c√°ch), I: indices (ch·ªâ s·ªë)\n",
        "    distances, indices = faiss_index.search(query_embedding_reshaped, k)\n",
        "\n",
        "    results = []\n",
        "    # L·∫∑p qua c√°c ch·ªâ s·ªë tr·∫£ v·ªÅ ƒë·ªÉ l·∫•y n·ªôi dung chunk v√† kho·∫£ng c√°ch/ƒë·ªô t∆∞∆°ng ƒë·ªìng\n",
        "    for i in range(k):\n",
        "        chunk_index = indices[0][i]\n",
        "        distance = distances[0][i]\n",
        "        results.append((chunks[chunk_index], distance)) # Tr·∫£ v·ªÅ chunk text v√† kho·∫£ng c√°ch\n",
        "\n",
        "    return results\n"
      ],
      "metadata": {
        "id": "SXtswqWyY0ga"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. T·∫°o ph·∫£n h·ªìi b·∫±ng Gemini API"
      ],
      "metadata": {
        "id": "tPMPk7FkZEsQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_response_with_gemini(query, retrieved_chunks):\n",
        "    \"\"\"\n",
        "    S·ª≠ d·ª•ng Gemini API ƒë·ªÉ t·∫°o c√¢u tr·∫£ l·ªùi d·ª±a tr√™n c√¢u h·ªèi v√† c√°c chunks ƒë∆∞·ª£c truy xu·∫•t.\n",
        "    \"\"\"\n",
        "    if not retrieved_chunks:\n",
        "        return \"Xin l·ªói, t√¥i kh√¥ng t√¨m th·∫•y th√¥ng tin li√™n quan trong s√°ch gi√°o khoa ƒë·ªÉ tr·∫£ l·ªùi c√¢u h·ªèi n√†y.\"\n",
        "\n",
        "    # N·ªëi c√°c n·ªôi dung chunk ƒë√£ truy xu·∫•t ƒë·ªÉ l√†m ng·ªØ c·∫£nh\n",
        "    context_text = \"\\n\\n\".join([chunk[0] for chunk in retrieved_chunks])\n",
        "\n",
        "    # X√¢y d·ª±ng prompt h∆∞·ªõng d·∫´n cho m√¥ h√¨nh ng√¥n ng·ªØ\n",
        "    prompt = f\"\"\"\n",
        "    B·∫°n l√† m·ªôt gia s∆∞ To√°n l·ªõp 6 th√¢n thi·ªán, ki√™n nh·∫´n v√† d·ªÖ hi·ªÉu.\n",
        "    H√£y tr·∫£ l·ªùi c√¢u h·ªèi c·ªßa h·ªçc sinh m·ªôt c√°ch s√∫c t√≠ch, r√µ r√†ng, ch·ªâ s·ª≠ d·ª•ng th√¥ng tin t·ª´ c√°c ƒëo·∫°n tr√≠ch s√°ch gi√°o khoa d∆∞·ªõi ƒë√¢y.\n",
        "    ƒê·∫£m b·∫£o ng√¥n ng·ªØ ph√π h·ª£p v·ªõi h·ªçc sinh l·ªõp 6.\n",
        "    N·∫øu th√¥ng tin kh√¥ng c√≥ trong s√°ch gi√°o khoa, h√£y n√≥i r√µ r·∫±ng b·∫°n kh√¥ng t√¨m th·∫•y c√¢u tr·∫£ l·ªùi trong t√†i li·ªáu n√†y.\n",
        "\n",
        "    C√°c ƒëo·∫°n tr√≠ch s√°ch gi√°o khoa li√™n quan:\n",
        "    ---\n",
        "    {context_text}\n",
        "    ---\n",
        "\n",
        "    C√¢u h·ªèi c·ªßa h·ªçc sinh: {query}\n",
        "    \"\"\"\n",
        "\n",
        "    try:\n",
        "        # Kh·ªüi t·∫°o m√¥ h√¨nh Gemini\n",
        "        model = genai.GenerativeModel('gemini-2.5-flash')\n",
        "\n",
        "        # T·∫°o ph·∫£n h·ªìi\n",
        "        response = model.generate_content(prompt)\n",
        "        return response.text\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå L·ªói khi g·ªçi Gemini API: {e}\")\n",
        "        return \"Xin l·ªói, c√≥ l·ªói x·∫£y ra khi t·∫°o c√¢u tr·∫£ l·ªùi. Vui l√≤ng ki·ªÉm tra API key ho·∫∑c k·∫øt n·ªëi m·∫°ng.\"\n",
        "\n",
        "# --- B·∫ÆT ƒê·∫¶U CH·∫†Y H·ªÜ TH·ªêNG RAG C·ª¶A B·∫†N ---\n",
        "text_file_path = '/content/drive/MyDrive/KhoÃÅa luaÃ£ÃÇn/Chatbot toaÃÅn/chatbot/sach_giao_khoa_lop_6_toan_tap1_text/noi_dung_sach_lop_6_CTST_tap1.txt' # ƒê·∫£m b·∫£o file n√†y c√≥ trong c√πng th∆∞ m·ª•c\n",
        "all_chunks = load_and_chunk_text(text_file_path)\n",
        "\n",
        "# T·∫°o embeddings cho t·∫•t c·∫£ c√°c chunks\n",
        "all_chunk_embeddings = generate_embeddings(all_chunks)\n",
        "\n",
        "# Kh·ªüi t·∫°o ch·ªâ m·ª•c FAISS\n",
        "faiss_index = None\n",
        "if all_chunk_embeddings is not None:\n",
        "    faiss_index = create_faiss_index(all_chunk_embeddings)\n",
        "else:\n",
        "    print(\"Kh√¥ng th·ªÉ t·∫°o ch·ªâ m·ª•c FAISS v√¨ kh√¥ng c√≥ embeddings. Robot AI c√≥ th·ªÉ kh√¥ng ho·∫°t ƒë·ªông.\")\n",
        "\n",
        "print(\"\\n--- Robot AI d·∫°y h·ªçc ƒë√£ s·∫µn s√†ng! ---\")\n",
        "print(\"Nh·∫≠p 'tho√°t' ƒë·ªÉ d·ª´ng.\")\n",
        "\n",
        "# --- V√≤ng l·∫∑p t∆∞∆°ng t√°c v·ªõi ng∆∞·ªùi d√πng  ---\n",
        "while True:\n",
        "    student_query = input(\"\\nH·ªçc sinh h·ªèi: \")\n",
        "    if student_query.lower() == 'tho√°t':\n",
        "        print(\"T·∫°m bi·ªát! H·∫πn g·∫∑p l·∫°i em.\")\n",
        "        break\n",
        "\n",
        "    if faiss_index is None or not all_chunks:\n",
        "        print(\"Robot AI kh√¥ng th·ªÉ ho·∫°t ƒë·ªông. Vui l√≤ng ki·ªÉm tra c√°c l·ªói kh·ªüi t·∫°o ·ªü tr√™n.\")\n",
        "        continue\n",
        "\n",
        "    # T·∫°o embedding cho c√¢u h·ªèi c·ªßa h·ªçc sinh\n",
        "    # D√íNG N√ÄY ƒê√É ƒê∆Ø·ª¢C S·ª¨A: B·ªè tham s·ªë 'show_progress_bar=False'\n",
        "    query_embedding = generate_embeddings([student_query])\n",
        "    if query_embedding is None or len(query_embedding) == 0:\n",
        "        print(\"‚ö†Ô∏è L·ªói: Kh√¥ng th·ªÉ t·∫°o embedding cho c√¢u h·ªèi c·ªßa b·∫°n. Vui l√≤ng th·ª≠ l·∫°i.\")\n",
        "        continue\n",
        "    else:\n",
        "        query_embedding = query_embedding[0]\n",
        "\n",
        "    retrieved_chunks = retrieve_top_k_chunks_faiss(query_embedding, faiss_index, all_chunks, k=5)\n",
        "    ai_response = generate_response_with_gemini(student_query, retrieved_chunks)\n",
        "\n",
        "    print(f\"Robot AI tr·∫£ l·ªùi: {ai_response}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G7w0xBILZDsH",
        "outputId": "94161b78-f2c5-46d9-ef9b-0c40627ab3a2"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ ƒê√£ t·∫£i v√† t·∫°o 1060 chunks t·ª´ file.\n",
            "‚è≥ ƒêang t·∫£i m√¥ h√¨nh SentenceTransformer: sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2...\n",
            "‚è≥ ƒêang t·∫°o embeddings cho 1060 chunks...\n",
            "‚úÖ ƒê√£ t·∫°o embeddings xong.\n",
            "‚úÖ ƒê√£ t·∫°o ch·ªâ m·ª•c FAISS v·ªõi 1060 vector.\n",
            "\n",
            "--- Robot AI d·∫°y h·ªçc ƒë√£ s·∫µn s√†ng! ---\n",
            "Nh·∫≠p 'tho√°t' ƒë·ªÉ d·ª´ng.\n",
            "‚è≥ ƒêang t·∫£i m√¥ h√¨nh SentenceTransformer: sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2...\n",
            "‚è≥ ƒêang t·∫°o embeddings cho 1 chunks...\n",
            "‚úÖ ƒê√£ t·∫°o embeddings xong.\n",
            "Robot AI tr·∫£ l·ªùi: Ch√†o con! R·∫•t vui ƒë∆∞·ª£c gi√∫p con h·ªçc To√°n nh√©.\n",
            "\n",
            "D·ª±a v√†o c√°c ƒëo·∫°n tr√≠ch s√°ch gi√°o khoa m√† con c√≥, th·∫ßy/c√¥ th·∫•y r·∫±ng:\n",
            "\n",
            "**1. D·∫•u hi·ªáu chia h·∫øt cho 9:**\n",
            "*   C√°c s·ªë c√≥ t·ªïng c√°c ch·ªØ s·ªë chia h·∫øt cho 9 th√¨ chia h·∫øt cho 9. V√† ch·ªâ nh·ªØng s·ªë ƒë√≥ m·ªõi chia h·∫øt cho 9.\n",
            "    *   V√≠ d·ª•: ƒê·ªÉ ki·ªÉm tra s·ªë 378 c√≥ chia h·∫øt cho 9 kh√¥ng, con t√≠nh t·ªïng c√°c ch·ªØ s·ªë: 3 + 7 + 8 = 18. V√¨ 18 chia h·∫øt cho 9, n√™n s·ªë 378 chia h·∫øt cho 9.\n",
            "\n",
            "**2. D·∫•u hi·ªáu chia h·∫øt cho 3:**\n",
            "*   Th·∫ßy/c√¥ kh√¥ng t√¨m th·∫•y d·∫•u hi·ªáu c·ª• th·ªÉ v·ªÅ chia h·∫øt cho 3 trong t√†i li·ªáu n√†y con ·∫°. ƒêo·∫°n vƒÉn \"2. D·∫•u hi·ªáu chia h·∫øt cho 3\" c√≥ v·∫ª nh∆∞ ch∆∞a ƒë∆∞·ª£c ƒë·∫ßy ƒë·ªß th√¥ng tin v·ªÅ quy t·∫Øc ƒë√≥.\n",
            "\n",
            "Hy v·ªçng ƒëi·ªÅu n√†y gi√∫p con r√µ h∆°n v·ªÅ d·∫•u hi·ªáu chia h·∫øt cho 9 nh√©! N·∫øu c√≥ c√¢u h·ªèi kh√°c, c·ª© h·ªèi th·∫ßy/c√¥!\n",
            "‚è≥ ƒêang t·∫£i m√¥ h√¨nh SentenceTransformer: sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2...\n",
            "‚è≥ ƒêang t·∫°o embeddings cho 1 chunks...\n",
            "‚úÖ ƒê√£ t·∫°o embeddings xong.\n",
            "Robot AI tr·∫£ l·ªùi: Ch√†o b·∫°n! ƒê√¢y l√† m·ªôt c√¢u h·ªèi r·∫•t hay v·ªÅ d·∫•u hi·ªáu chia h·∫øt cho 9 ƒë√≥.\n",
            "\n",
            "ƒê·ªÉ bi·∫øt s·ªë 159 c√≥ chia h·∫øt cho 9 hay kh√¥ng, ch√∫ng ta s·∫Ω √°p d·ª•ng quy t·∫Øc m√† s√°ch gi√°o khoa ƒë√£ h∆∞·ªõng d·∫´n nh√©:\n",
            "\n",
            "1.  **T√≠nh t·ªïng c√°c ch·ªØ s·ªë c·ªßa s·ªë 159:**\n",
            "    C√°c ch·ªØ s·ªë c·ªßa 159 l√† 1, 5 v√† 9.\n",
            "    T·ªïng c·ªßa ch√∫ng l√†: 1 + 5 + 9 = 15.\n",
            "\n",
            "2.  **Ki·ªÉm tra xem t·ªïng n√†y c√≥ chia h·∫øt cho 9 kh√¥ng:**\n",
            "    S·ªë 15 c√≥ chia h·∫øt cho 9 kh√¥ng nh·ªâ?\n",
            "    N·∫øu b·∫°n ƒë·∫øm theo b·∫£ng c·ª≠u ch∆∞∆°ng 9, b·∫°n s·∫Ω th·∫•y 9 x 1 = 9, 9 x 2 = 18. S·ªë 15 kh√¥ng n·∫±m trong d√£y n√†y.\n",
            "    V·∫≠y, 15 kh√¥ng chia h·∫øt cho 9.\n",
            "\n",
            "V√¨ t·ªïng c√°c ch·ªØ s·ªë c·ªßa 159 (l√† 15) kh√¥ng chia h·∫øt cho 9, n√™n theo d·∫•u hi·ªáu chia h·∫øt cho 9, **s·ªë 159 kh√¥ng chia h·∫øt cho 9** b·∫°n nh√©!\n",
            "\n",
            "B·∫°n ƒë√£ hi·ªÉu c√°ch l√†m ch∆∞a? N·∫øu c√≥ c√¢u h·ªèi kh√°c, c·ª© h·ªèi m√¨nh nh√©!\n",
            "‚è≥ ƒêang t·∫£i m√¥ h√¨nh SentenceTransformer: sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2...\n",
            "‚è≥ ƒêang t·∫°o embeddings cho 1 chunks...\n",
            "‚úÖ ƒê√£ t·∫°o embeddings xong.\n",
            "Robot AI tr·∫£ l·ªùi: Ch√†o con! ƒê√¢y l√† m·ªôt c√¢u h·ªèi r·∫•t hay.\n",
            "\n",
            "C√¥/Th·∫ßy ƒë√£ xem qua c√°c ƒëo·∫°n tr√≠ch s√°ch gi√°o khoa m√† con cung c·∫•p. C√¥/Th·∫ßy th·∫•y s√°ch c√≥ nh·∫Øc ƒë·∫øn \"D·∫•u hi·ªáu chia h·∫øt cho 3\" trong ph·∫ßn t·ª´ kh√≥a v√† gi·ªõi thi·ªáu ch·ªß ƒë·ªÅ. Tuy nhi√™n, s√°ch ch·ªâ tr√¨nh b√†y chi ti·∫øt v·ªÅ \"D·∫•u hi·ªáu chia h·∫øt cho 9\" m√† ch∆∞a c√≥ ph·∫ßn gi·∫£i th√≠ch c·ª• th·ªÉ v·ªÅ \"D·∫•u hi·ªáu chia h·∫øt cho 3\" ·ªü ƒë√¢y con ·∫°.\n",
            "\n",
            "V·∫≠y n√™n, c√¥/th·∫ßy kh√¥ng t√¨m th·∫•y c√¢u tr·∫£ l·ªùi ch√≠nh x√°c cho \"d·∫•u hi·ªáu chia h·∫øt cho 3\" trong t√†i li·ªáu n√†y.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7F-i_mCulRye",
        "outputId": "70dbd636-0636-4c81-b19b-8ab014915012"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import google.generativeai as genai\n",
        "import gradio as gr\n",
        "\n",
        "# --- H√†m sinh ph·∫£n h·ªìi ---\n",
        "def generate_response_with_gemini(query, model_choice):\n",
        "    retrieved_chunks = [(\"V√≠ d·ª• n·ªôi dung SGK To√°n 6\", 0)]  # d·ªØ li·ªáu m·∫´u\n",
        "    context_text = \"\\n\\n\".join([chunk[0] for chunk in retrieved_chunks])\n",
        "    prompt = f\"\"\"\n",
        "    B·∫°n l√† gia s∆∞ To√°n th√¢n thi·ªán.\n",
        "    Tr·∫£ l·ªùi ng·∫Øn g·ªçn, d·ªÖ hi·ªÉu, ch·ªâ d√πng n·ªôi dung SGK:\n",
        "    ---\n",
        "    {context_text}\n",
        "    ---\n",
        "    C√¢u h·ªèi: {query}\n",
        "    \"\"\"\n",
        "    try:\n",
        "        model = genai.GenerativeModel(model_choice)\n",
        "        response = model.generate_content(prompt)\n",
        "        return response.text\n",
        "    except Exception as e:\n",
        "        return f\"L·ªói g·ªçi Gemini API: {e}\"\n",
        "\n",
        "# --- Giao di·ªán Gradio ---\n",
        "def chat_interface(query, model):\n",
        "    return generate_response_with_gemini(query, model)\n",
        "\n",
        "with gr.Blocks(theme=gr.themes.Soft()) as demo:\n",
        "    gr.Markdown(\"## ü§ñ Robot Gia S∆∞ To√°n  - Gemini RAG\")\n",
        "    with gr.Row():\n",
        "        query = gr.Textbox(label=\"Nh·∫≠p c√¢u h·ªèi c·ªßa em:\")\n",
        "        model = gr.Radio(\n",
        "            [\"gemini-2.5-flash\", \"gemini-2.0-pro\"],\n",
        "            label=\"Ch·ªçn m√¥ h√¨nh Gemini\",\n",
        "            value=\"gemini-2.5-flash\"\n",
        "        )\n",
        "    answer = gr.Textbox(label=\"C√¢u tr·∫£ l·ªùi\", lines=10)\n",
        "    submit_btn = gr.Button(\"üéì G·ª≠i c√¢u h·ªèi\")\n",
        "    submit_btn.click(fn=chat_interface, inputs=[query, model], outputs=answer)\n",
        "\n",
        "demo.launch()\n"
      ],
      "metadata": {
        "id": "4B52324xxAQU",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 626
        },
        "outputId": "5d3e0f71-2969-47fe-9c2b-13fecbba0a33"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It looks like you are running Gradio on a hosted Jupyter notebook, which requires `share=True`. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://951f3a5414cd94f8a7.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://951f3a5414cd94f8a7.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "m9td5BGm8owC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}